{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install Libraries\n",
        "# !pip install --upgrade mlflow azureml-mlflow scikit-learn\n",
        "# !pip install scikit-learn"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1753889336343
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Hybrid Recommendation Model with Azure AutoML and MovieLens Dataset\n",
        "\n",
        "# The workspace information from the previous experiment has been pre-filled for you.\n",
        "subscription_id = \"c266be78-fe83-4597-afd1-5a4ea8eda621\"\n",
        "resource_group = \"AzureML\"\n",
        "workspace_name = \"AzureML\"\n",
        "\n",
        "\"\"\"\n",
        "Learning objectives:\n",
        "1. Extract user and product factors from a matrix factorization model using scikit-learn\n",
        "2. Format inputs for Azure AutoML hybrid recommendation model\n",
        "3. Train and deploy a hybrid recommendation system on Azure ML\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook demonstrates how to build a hybrid recommendation system that combines:\n",
        "- Matrix factorization user/item embeddings \n",
        "- User metadata (synthetic loyalty, location data)\n",
        "- Item metadata (movie genres, titles)\n",
        "\n",
        "The approach uses Azure AutoML to automatically select and tune the best regression model\n",
        "for predicting ratings based on these combined features.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import os\n",
        "import joblib\n",
        "\n",
        "# Azure ML imports\n",
        "from azureml.core import Workspace, Dataset, Experiment, Environment\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "from azureml.core.model import Model\n",
        "from azureml.core.webservice import AciWebservice\n",
        "import logging\n",
        "\n",
        "# Set your Azure ML workspace details\n",
        "SUBSCRIPTION_ID = subscription_id\n",
        "RESOURCE_GROUP = resource_group\n",
        "WORKSPACE_NAME = workspace_name\n",
        "EXPERIMENT_NAME = \"hybrid-recommendation-automl\"\n",
        "\n",
        "# Initialize workspace\n",
        "ws = Workspace(subscription_id=SUBSCRIPTION_ID,\n",
        "               resource_group=RESOURCE_GROUP,\n",
        "               workspace_name=WORKSPACE_NAME)\n",
        "\n",
        "print(f\"Workspace: {ws.name}\")\n",
        "print(f\"Region: {ws.location}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Workspace: AzureML\nRegion: centralus\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1753889337625
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 1: Load and Prepare MovieLens Data\n",
        "def load_movielens_data():\n",
        "    \"\"\"\n",
        "    Load MovieLens 20M dataset from CSV files\n",
        "    Expected files: ratings.csv, movies.csv\n",
        "    \"\"\"\n",
        "    # Load ratings data\n",
        "    ratings_df = pd.read_csv('ratings.csv')\n",
        "    print(f\"Loaded {len(ratings_df)} ratings\")\n",
        "    \n",
        "    # Load movies data  \n",
        "    movies_df = pd.read_csv('movies.csv')\n",
        "    print(f\"Loaded {len(movies_df)} movies\")\n",
        "    \n",
        "    # Parse genres into lists\n",
        "    movies_df['genres_list'] = movies_df['genres'].str.split('|')\n",
        "    \n",
        "    return ratings_df, movies_df\n",
        "\n",
        "def create_synthetic_user_features(ratings_df):\n",
        "    \"\"\"\n",
        "    Create synthetic user features since MovieLens doesn't include user demographics\n",
        "    \"\"\"\n",
        "    user_stats = ratings_df.groupby('userId').agg({\n",
        "        'rating': ['count', 'mean', 'std']\n",
        "    }).reset_index()\n",
        "    \n",
        "    user_stats.columns = ['userId', 'num_ratings', 'avg_rating', 'rating_std']\n",
        "    user_stats['rating_std'] = user_stats['rating_std'].fillna(0)\n",
        "    \n",
        "    # Create synthetic features\n",
        "    np.random.seed(42)\n",
        "    user_stats['loyalty'] = np.random.rand(len(user_stats)) * user_stats['num_ratings']\n",
        "    user_stats['postcode'] = (user_stats['userId'] % 100).astype(str).str.zfill(2)\n",
        "    user_stats['age_group'] = np.random.choice(['18-25', '26-35', '36-45', '46-55', '55+'], \n",
        "                                               len(user_stats))\n",
        "    \n",
        "    return user_stats"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1753889337685
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 2: Train Matrix Factorization Model\n",
        "def train_matrix_factorization(ratings_df, n_factors=16, sample_size=50000):\n",
        "    \"\"\"\n",
        "    Train matrix factorization model using sparse matrices and sampling for memory efficiency\n",
        "    \"\"\"\n",
        "    print(\"Training matrix factorization model...\")\n",
        "    \n",
        "    # Sample data if dataset is too large\n",
        "    # if len(ratings_df) > sample_size:\n",
        "    #     print(f\"Sampling {sample_size} ratings from {len(ratings_df)} total ratings\")\n",
        "    #     ratings_sample = ratings_df.sample(n=sample_size, random_state=42)\n",
        "    # else:\n",
        "    #     ratings_sample = ratings_df.copy()\n",
        "    ratings_sample = ratings_df.copy()\n",
        "    # Use only active users and movies to reduce matrix size\n",
        "    user_counts = ratings_sample['userId'].value_counts()\n",
        "    movie_counts = ratings_sample['movieId'].value_counts()\n",
        "    \n",
        "    # Keep users with at least 10 ratings and movies with at least 5 ratings\n",
        "    active_users = user_counts[user_counts >= 10].index\n",
        "    active_movies = movie_counts[movie_counts >= 5].index\n",
        "    \n",
        "    # Filter to active users and movies\n",
        "    filtered_ratings = ratings_sample[\n",
        "        (ratings_sample['userId'].isin(active_users)) & \n",
        "        (ratings_sample['movieId'].isin(active_movies))\n",
        "    ]\n",
        "    \n",
        "    print(f\"Filtered to {len(active_users)} users and {len(active_movies)} movies\")\n",
        "    print(f\"Matrix size: {len(active_users)} x {len(active_movies)} = {len(active_users) * len(active_movies):,} cells\")\n",
        "    \n",
        "    # Create sparse user-item matrix using scipy\n",
        "    from scipy.sparse import csr_matrix\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    \n",
        "    # Encode users and movies to continuous indices\n",
        "    user_encoder = LabelEncoder()\n",
        "    movie_encoder = LabelEncoder()\n",
        "    \n",
        "    filtered_ratings['user_idx'] = user_encoder.fit_transform(filtered_ratings['userId'])\n",
        "    filtered_ratings['movie_idx'] = movie_encoder.fit_transform(filtered_ratings['movieId'])\n",
        "    \n",
        "    # Create sparse matrix\n",
        "    n_users = len(active_users)\n",
        "    n_movies = len(active_movies)\n",
        "    \n",
        "    user_item_sparse = csr_matrix(\n",
        "        (filtered_ratings['rating'].values, \n",
        "         (filtered_ratings['user_idx'].values, filtered_ratings['movie_idx'].values)),\n",
        "        shape=(n_users, n_movies)\n",
        "    )\n",
        "    \n",
        "    print(f\"Sparse matrix created with {user_item_sparse.nnz:,} non-zero entries\")\n",
        "    print(f\"Sparsity: {(1 - user_item_sparse.nnz / (n_users * n_movies)) * 100:.2f}%\")\n",
        "    \n",
        "    # --- FIX: Removed alpha and l1_ratio to match scikit-learn v0.16 ---\n",
        "    # Train NMF model on sparse matrix\n",
        "    nmf_model = NMF(n_components=n_factors, \n",
        "                    init='random', \n",
        "                    random_state=42,\n",
        "                    max_iter=100)  # Reduced iterations for speed\n",
        "    \n",
        "    # Fit and transform to get user factors\n",
        "    print(\"Fitting NMF model...\")\n",
        "    user_factors = nmf_model.fit_transform(user_item_sparse)\n",
        "    \n",
        "    # Get item factors\n",
        "    item_factors = nmf_model.components_.T\n",
        "    \n",
        "    # Create DataFrames for factors with original IDs\n",
        "    user_factors_df = pd.DataFrame(user_factors, \n",
        "                                  columns=[f'user_factor_{i+1}' for i in range(n_factors)])\n",
        "    user_factors_df['userId'] = user_encoder.inverse_transform(range(n_users))\n",
        "    \n",
        "    item_factors_df = pd.DataFrame(item_factors,\n",
        "                                  columns=[f'item_factor_{i+1}' for i in range(n_factors)])\n",
        "    item_factors_df['movieId'] = movie_encoder.inverse_transform(range(n_movies))\n",
        "    \n",
        "    print(f\"User factors shape: {user_factors_df.shape}\")\n",
        "    print(f\"Item factors shape: {item_factors_df.shape}\")\n",
        "    \n",
        "    return user_factors_df, item_factors_df, nmf_model"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1753889338083
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 3: Create Hybrid Dataset\n",
        "\n",
        "def create_hybrid_dataset(ratings_df, movies_df, user_features_df, \n",
        "                         user_factors_df, item_factors_df, max_samples=100000):\n",
        "    \"\"\"\n",
        "    Combine ratings with user factors, item factors, and metadata\n",
        "    Memory-efficient version with sampling\n",
        "    \"\"\"\n",
        "    print(\"Creating hybrid dataset...\")\n",
        "    \n",
        "    # Sample ratings if too large\n",
        "    if len(ratings_df) > max_samples:\n",
        "        print(f\"Sampling {max_samples} ratings from {len(ratings_df)} total\")\n",
        "        ratings_sample = ratings_df.sample(n=max_samples, random_state=42)\n",
        "    else:\n",
        "        ratings_sample = ratings_df.copy()\n",
        "    \n",
        "    # Start with sampled ratings\n",
        "    hybrid_df = ratings_sample.copy()\n",
        "    \n",
        "    # Add user factors (only for users that exist in factors)\n",
        "    hybrid_df = hybrid_df.merge(user_factors_df, on='userId', how='inner')\n",
        "    print(f\"After user factors merge: {len(hybrid_df)} rows\")\n",
        "    \n",
        "    # Add user features\n",
        "    hybrid_df = hybrid_df.merge(user_features_df, on='userId', how='left')\n",
        "    print(f\"After user features merge: {len(hybrid_df)} rows\")\n",
        "    \n",
        "    # Add item factors (only for items that exist in factors)\n",
        "    hybrid_df = hybrid_df.merge(item_factors_df, on='movieId', how='inner')\n",
        "    print(f\"After item factors merge: {len(hybrid_df)} rows\")\n",
        "    \n",
        "    # Add movie metadata\n",
        "    hybrid_df = hybrid_df.merge(movies_df[['movieId', 'title', 'genres']], \n",
        "                               on='movieId', how='left')\n",
        "    print(f\"After movie metadata merge: {len(hybrid_df)} rows\")\n",
        "    \n",
        "    # Simplified genre handling - just use first genre\n",
        "    hybrid_df['primary_genre'] = hybrid_df['genres'].str.split('|').str[0]\n",
        "    hybrid_df['primary_genre'] = hybrid_df['primary_genre'].fillna('Unknown')\n",
        "    \n",
        "    # Encode categorical variables\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    \n",
        "    le_genre = LabelEncoder()\n",
        "    hybrid_df['genre_encoded'] = le_genre.fit_transform(hybrid_df['primary_genre'])\n",
        "    \n",
        "    le_postcode = LabelEncoder()\n",
        "    hybrid_df['postcode_encoded'] = le_postcode.fit_transform(hybrid_df['postcode'])\n",
        "    \n",
        "    le_age = LabelEncoder()\n",
        "    hybrid_df['age_group_encoded'] = le_age.fit_transform(hybrid_df['age_group'])\n",
        "    \n",
        "    # Select features for training\n",
        "    feature_columns = (['userId', 'movieId'] + \n",
        "                      [f'user_factor_{i+1}' for i in range(16)] +\n",
        "                      [f'item_factor_{i+1}' for i in range(16)] +\n",
        "                      ['loyalty', 'num_ratings', 'avg_rating', 'rating_std',\n",
        "                       'genre_encoded', 'postcode_encoded', 'age_group_encoded'])\n",
        "    \n",
        "    final_df = hybrid_df[feature_columns + ['rating']].copy()\n",
        "    \n",
        "    # Remove any rows with missing values\n",
        "    final_df = final_df.dropna()\n",
        "    \n",
        "    print(f\"Final hybrid dataset shape: {final_df.shape}\")\n",
        "    print(f\"Memory usage: {final_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "    \n",
        "    return final_df, le_genre, le_postcode, le_age"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1753889338145
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 4: Prepare Data for Azure AutoML\n",
        "\n",
        "def prepare_automl_data(hybrid_df, ws):\n",
        "    \"\"\"\n",
        "    Prepare and register dataset for Azure AutoML\n",
        "    \"\"\"\n",
        "    print(\"Preparing data for Azure AutoML...\")\n",
        "    \n",
        "    # Split data\n",
        "    train_df, test_df = train_test_split(hybrid_df, test_size=0.2, random_state=42)\n",
        "    \n",
        "    # Save to CSV\n",
        "    train_df.to_csv('train_data.csv', index=False)\n",
        "    test_df.to_csv('test_data.csv', index=False)\n",
        "    \n",
        "    # Register datasets in Azure ML\n",
        "    datastore = ws.get_default_datastore()\n",
        "    \n",
        "    # Upload and register training dataset\n",
        "    train_dataset = Dataset.Tabular.from_delimited_files(\n",
        "        path=datastore.upload_files(['train_data.csv'], \n",
        "                                   target_path='hybrid-recommendation/'),\n",
        "        validate=True,\n",
        "        include_path=False,\n",
        "        infer_column_types=True,\n",
        "        set_column_types=None,\n",
        "        separator=',',\n",
        "        header=True\n",
        "    )\n",
        "    \n",
        "    train_dataset = train_dataset.register(\n",
        "        workspace=ws,\n",
        "        name='hybrid-recommendation-train',\n",
        "        description='Training data for hybrid recommendation model',\n",
        "        create_new_version=True\n",
        "    )\n",
        "    \n",
        "    # Upload and register test dataset\n",
        "    test_dataset = Dataset.Tabular.from_delimited_files(\n",
        "        path=datastore.upload_files(['test_data.csv'], \n",
        "                                   target_path='hybrid-recommendation/'),\n",
        "        validate=True,\n",
        "        include_path=False,\n",
        "        infer_column_types=True,\n",
        "        separator=',',\n",
        "        header=True\n",
        "    )\n",
        "    \n",
        "    test_dataset = test_dataset.register(\n",
        "        workspace=ws,\n",
        "        name='hybrid-recommendation-test',\n",
        "        description='Test data for hybrid recommendation model',\n",
        "        create_new_version=True\n",
        "    )\n",
        "    \n",
        "    return train_dataset, test_dataset, train_df, test_df"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1753889338208
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 5: Configure and Run AutoML\n",
        "\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException # Good practice to catch a specific exception\n",
        "\n",
        "def setup_compute_target(ws, compute_name=\"hybrid-rec-compute\"):\n",
        "    \"\"\"\n",
        "    Create or retrieve compute target for AutoML and wait for it to be ready.\n",
        "    \"\"\"\n",
        "    # try:\n",
        "    #     compute_target = ComputeTarget(workspace=ws, name=compute_name)\n",
        "    #     print(f\"Found existing compute target: {compute_name}\")\n",
        "        \n",
        "    #     # --- FIX: Wait for the existing compute to be ready ---\n",
        "    #     # This is crucial for clusters with min_nodes=0, as it waits for scaling up.\n",
        "    #     print(\"Waiting for the compute target to be ready...\")\n",
        "    #     compute_target.wait_for_completion(show_output=True)\n",
        "    #     print(\"Compute target is ready.\")\n",
        "\n",
        "    # except ComputeTargetException:\n",
        "    print(f\"Creating new compute target: {compute_name}\")\n",
        "    compute_config = AmlCompute.provisioning_configuration(\n",
        "            vm_size=\"STANDARD_A4M_V2\",\n",
        "            min_nodes=0,\n",
        "            max_nodes=1,\n",
        "            idle_seconds_before_scaledown=300\n",
        "        )\n",
        "    compute_target = ComputeTarget.create(ws, compute_name, compute_config)\n",
        "        \n",
        "    # This wait is for the initial creation of the cluster\n",
        "    compute_target.wait_for_completion(show_output=True)\n",
        "    \n",
        "    return compute_target"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1753889338270
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def configure_automl(train_dataset, compute_target):\n",
        "    \"\"\"\n",
        "    Configure AutoML for regression task\n",
        "    \"\"\"\n",
        "    automl_config = AutoMLConfig(\n",
        "        task='regression',\n",
        "        primary_metric='normalized_root_mean_squared_error',\n",
        "        training_data=train_dataset,\n",
        "        label_column_name='rating',\n",
        "        compute_target=compute_target,\n",
        "        experiment_timeout_minutes=60,\n",
        "        max_concurrent_iterations=4,\n",
        "        max_cores_per_iteration=-1,\n",
        "        enable_early_stopping=True,\n",
        "        n_cross_validations=5,\n",
        "        verbosity=logging.INFO,\n",
        "        enable_voting_ensemble=True,\n",
        "        enable_stack_ensemble=True,\n",
        "        # Feature engineering\n",
        "        enable_feature_sweeping=True,\n",
        "        featurization='auto',\n",
        "        # Model selection\n",
        "        blocked_models=['XGBoostRegressor'],  # This name is correct\n",
        "        allowed_models=['RandomForest', 'LightGBM', \n",
        "                        'ExtremeRandomTrees', 'GradientBoosting',\n",
        "                        'ElasticNet']\n",
        "    )\n",
        "    \n",
        "    return automl_config"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1753890004713
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_automl_experiment(ws, automl_config):\n",
        "    \"\"\"\n",
        "    Run AutoML experiment\n",
        "    \"\"\"\n",
        "    experiment = Experiment(ws, EXPERIMENT_NAME)\n",
        "    \n",
        "    print(\"Starting AutoML run...\")\n",
        "    automl_run = experiment.submit(automl_config, show_output=True, wait_post_processing=True)\n",
        "    \n",
        "    # Get best model\n",
        "    best_run, fitted_model = automl_run.get_output()\n",
        "    \n",
        "    print(f\"Best model: {fitted_model}\")\n",
        "    print(f\"Best run metrics:\")\n",
        "    print(best_run.get_metrics())\n",
        "    \n",
        "    return automl_run, best_run, fitted_model"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1753889338378
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 6: Evaluate and Deploy Model\n",
        "def evaluate_model(best_run, test_df):\n",
        "    \"\"\"\n",
        "    Evaluate the best model on test data\n",
        "    \"\"\"\n",
        "    print(\"Evaluating model on test data...\")\n",
        "    \n",
        "    # Download the model\n",
        "    model_path = best_run.download_file('outputs/model.pkl')\n",
        "    model = joblib.load(model_path)\n",
        "    \n",
        "    # Prepare test features\n",
        "    feature_columns = [col for col in test_df.columns if col != 'rating']\n",
        "    X_test = test_df[feature_columns]\n",
        "    y_test = test_df['rating']\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "    \n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    \n",
        "    print(f\"Test RMSE: {rmse:.4f}\")\n",
        "    print(f\"Test MAE: {mae:.4f}\")\n",
        "    print(f\"Test R²: {r2:.4f}\")\n",
        "    \n",
        "    return model, y_pred"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1753889338848
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def register_and_deploy_model(ws, best_run, model_name=\"hybrid-recommendation-model\"):\n",
        "    \"\"\"\n",
        "    Register and deploy the best model\n",
        "    \"\"\"\n",
        "    print(\"Registering model...\")\n",
        "    \n",
        "    # Register model\n",
        "    model = best_run.register_model(\n",
        "        model_name=model_name,\n",
        "        description=\"Hybrid recommendation model trained with AutoML\",\n",
        "        tags={\"type\": \"recommendation\", \"algorithm\": \"hybrid\"}\n",
        "    )\n",
        "    \n",
        "    print(f\"Model registered: {model.name} version {model.version}\")\n",
        "    \n",
        "    # Deploy as web service\n",
        "    print(\"Deploying model...\")\n",
        "    \n",
        "    # Create inference configuration\n",
        "    inference_config = best_run.get_inference_config()\n",
        "    \n",
        "    # Create deployment configuration\n",
        "    deployment_config = AciWebservice.deploy_configuration(\n",
        "        cpu_cores=1,\n",
        "        memory_gb=2,\n",
        "        description=\"Hybrid recommendation model endpoint\"\n",
        "    )\n",
        "    \n",
        "    # Deploy\n",
        "    service = Model.deploy(\n",
        "        workspace=ws,\n",
        "        name=f\"{model_name}-service\",\n",
        "        models=[model],\n",
        "        inference_config=inference_config,\n",
        "        deployment_config=deployment_config,\n",
        "        overwrite=True\n",
        "    )\n",
        "    \n",
        "    service.wait_for_deployment(show_output=True)\n",
        "    \n",
        "    print(f\"Service deployed at: {service.scoring_uri}\")\n",
        "    \n",
        "    return model, service"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1753889339303
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 7: Make Predictions\n",
        "def make_recommendations(service, user_id, user_factors_df, item_factors_df, \n",
        "                        user_features_df, movies_df, top_k=10):\n",
        "    \"\"\"\n",
        "    Make movie recommendations for a specific user\n",
        "    \"\"\"\n",
        "    # Get user features\n",
        "    user_data = user_features_df[user_features_df['userId'] == user_id].iloc[0]\n",
        "    user_factors = user_factors_df[user_factors_df['userId'] == user_id].iloc[0]\n",
        "    \n",
        "    # Get all movies the user hasn't rated\n",
        "    # (In practice, you'd filter out movies the user has already rated)\n",
        "    \n",
        "    recommendations = []\n",
        "    \n",
        "    # Sample some movies for demonstration\n",
        "    sample_movies = movies_df.sample(100)\n",
        "    \n",
        "    for _, movie in sample_movies.iterrows():\n",
        "        # Get movie factors\n",
        "        movie_factors = item_factors_df[item_factors_df['movieId'] == movie['movieId']]\n",
        "        \n",
        "        if len(movie_factors) > 0:\n",
        "            movie_factors = movie_factors.iloc[0]\n",
        "            \n",
        "            # Create input features\n",
        "            features = {\n",
        "                'userId': user_id,\n",
        "                'movieId': movie['movieId'],\n",
        "                'loyalty': user_data['loyalty'],\n",
        "                'num_ratings': user_data['num_ratings'],\n",
        "                'avg_rating': user_data['avg_rating'],\n",
        "                'rating_std': user_data['rating_std'],\n",
        "                # Add user factors\n",
        "                **{f'user_factor_{i+1}': user_factors[f'user_factor_{i+1}'] \n",
        "                   for i in range(16)},\n",
        "                # Add item factors  \n",
        "                **{f'item_factor_{i+1}': movie_factors[f'item_factor_{i+1}'] \n",
        "                   for i in range(16)},\n",
        "                # Add encoded features (simplified for demo)\n",
        "                'genre_encoded': 0,  # Would need proper encoding\n",
        "                'postcode_encoded': user_data.get('postcode_encoded', 0),\n",
        "                'age_group_encoded': user_data.get('age_group_encoded', 0)\n",
        "            }\n",
        "            \n",
        "            # Make prediction via web service\n",
        "            import json\n",
        "            input_data = json.dumps({'data': [list(features.values())]})\n",
        "            \n",
        "            try:\n",
        "                prediction = service.run(input_data)\n",
        "                predicted_rating = float(prediction[0])\n",
        "                \n",
        "                recommendations.append({\n",
        "                    'movieId': movie['movieId'],\n",
        "                    'title': movie['title'],\n",
        "                    'predicted_rating': predicted_rating\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Error predicting for movie {movie['movieId']}: {e}\")\n",
        "    \n",
        "    # Sort by predicted rating\n",
        "    recommendations.sort(key=lambda x: x['predicted_rating'], reverse=True)\n",
        "    \n",
        "    return recommendations[:top_k]"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1753889339362
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load data\n",
        "print(\"=== Step 1: Loading Data ===\")\n",
        "ratings_df, movies_df = load_movielens_data()\n",
        "user_features_df = create_synthetic_user_features(ratings_df)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "=== Step 1: Loading Data ===\nLoaded 20000263 ratings\nLoaded 27278 movies\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1753889347628
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Train matrix factorization\n",
        "print(\"\\n=== Step 2: Training Matrix Factorization ===\")\n",
        "user_factors_df, item_factors_df, nmf_model = train_matrix_factorization(ratings_df)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n=== Step 2: Training Matrix Factorization ===\nTraining matrix factorization model...\nFiltered to 138493 users and 18345 movies\nMatrix size: 138493 x 18345 = 2,540,654,085 cells\nSparse matrix created with 19,984,024 non-zero entries\nSparsity: 99.21%\nFitting NMF model...\nUser factors shape: (138493, 17)\nItem factors shape: (18345, 17)\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_15076/2589040002.py:40: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_ratings['user_idx'] = user_encoder.fit_transform(filtered_ratings['userId'])\n/tmp/ipykernel_15076/2589040002.py:41: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_ratings['movie_idx'] = movie_encoder.fit_transform(filtered_ratings['movieId'])\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1753889394365
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create hybrid dataset\n",
        "print(\"\\n=== Step 3: Creating Hybrid Dataset ===\")\n",
        "hybrid_df, le_genre, le_postcode, le_age = create_hybrid_dataset(\n",
        "            ratings_df, movies_df, user_features_df, user_factors_df, item_factors_df\n",
        "        )"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n=== Step 3: Creating Hybrid Dataset ===\nCreating hybrid dataset...\nSampling 100000 ratings from 20000263 total\nAfter user factors merge: 100000 rows\nAfter user features merge: 100000 rows\nAfter item factors merge: 99910 rows\nAfter movie metadata merge: 99910 rows\nFinal hybrid dataset shape: (99910, 42)\nMemory usage: 32.8 MB\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1753889395671
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Prepare for AutoML\n",
        "print(\"\\n=== Step 4: Preparing AutoML Data ===\")\n",
        "train_dataset, test_dataset, train_df, test_df = prepare_automl_data(hybrid_df, ws)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n=== Step 4: Preparing AutoML Data ===\nPreparing data for Azure AutoML...\nUploading an estimated of 1 files\nTarget already exists. Skipping upload for hybrid-recommendation/train_data.csv\nUploaded 0 files\nUploading an estimated of 1 files\nTarget already exists. Skipping upload for hybrid-recommendation/test_data.csv\nUploaded 0 files\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1753889403099
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Setup and run AutoML\n",
        "# print(\"\\n=== Step 5: Running AutoML ===\")\n",
        "# compute_target = setup_compute_target(ws)\n",
        "# automl_config = configure_automl(train_dataset, compute_target)\n",
        "# automl_run, best_run, fitted_model = run_automl_experiment(ws, automl_config)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n=== Step 5: Running AutoML ===\nCreating new compute target: hybrid-rec-compute\nInProgress..\nSucceededProvisioning operation finished, operation \"Succeeded\"\nSucceeded\nAmlCompute wait for completion finished\n\nMinimum number of nodes requested have been provisioned\nStarting AutoML run...\nSubmitting remote run.\nNo run_configuration provided, running on hybrid-rec-compute with default configuration\nRunning on remote compute: hybrid-rec-compute\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>hybrid-recommendation-automl</td><td>AutoML_52c2a35d-2d37-434d-bdf3-dbe469298ed6</td><td>automl</td><td>NotStarted</td><td><a href=\"https://ml.azure.com/runs/AutoML_52c2a35d-2d37-434d-bdf3-dbe469298ed6?wsid=/subscriptions/c266be78-fe83-4597-afd1-5a4ea8eda621/resourcegroups/AzureML/workspaces/AzureML&amp;tid=7ee758ca-5953-4955-bf97-44997efd4c16\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nCurrent status: DatasetEvaluation. Gathering dataset statistics.\nCurrent status: FeaturesGeneration. Generating features for the dataset.\nCurrent status: DatasetFeaturization. Beginning to fit featurizers and featurize the dataset.\nCurrent status: DatasetCrossValidationSplit. Generating individually featurized CV splits.\nCurrent status: ModelSelection. Beginning model selection.\n\n********************************************************************************************\nDATA GUARDRAILS: \n\nTYPE:         Missing feature values imputation\nSTATUS:       PASSED\nDESCRIPTION:  No feature missing values were detected in the training data.\n              Learn more about missing value imputation: https://aka.ms/AutomatedMLFeaturization\n\n********************************************************************************************\n\nTYPE:         High cardinality feature detection\nSTATUS:       PASSED\nDESCRIPTION:  Your inputs were analyzed, and no high cardinality features were detected.\n              Learn more about high cardinality feature handling: https://aka.ms/AutomatedMLFeaturization\n\n********************************************************************************************\n\n********************************************************************************************\nITER: The iteration being evaluated.\nPIPELINE: A summary description of the pipeline being evaluated.\nDURATION: Time taken for the current iteration.\nMETRIC: The result of computing score on the fitted pipeline.\nBEST: The best observed score thus far.\n********************************************************************************************\n\n ITER   PIPELINE                                       DURATION            METRIC      BEST\n    0   MaxAbsScaler LightGBM                          0:02:28             0.1898    0.1898\n    1   MaxAbsScaler ElasticNet                        0:00:27             0.1989    0.1898\n    2   StandardScalerWrapper ElasticNet               0:00:44             0.1987    0.1898\n    3   MaxAbsScaler ElasticNet                        0:00:27             0.1989    0.1898\n    4   MaxAbsScaler ElasticNet                        0:00:28             0.1989    0.1898\n    5   MaxAbsScaler ExtremeRandomTrees                0:02:10             0.1973    0.1898\n    6   StandardScalerWrapper ElasticNet               0:00:21             0.2017    0.1898\n    7   StandardScalerWrapper ElasticNet               0:01:47             0.1987    0.1898\n    8   StandardScalerWrapper ElasticNet               0:00:28             0.2141    0.1898\n    9   MaxAbsScaler RandomForest                      0:00:46             0.2054    0.1898\n   10   StandardScalerWrapper ElasticNet               0:00:28             0.2044    0.1898\n   11   StandardScalerWrapper ElasticNet               0:00:24             0.2034    0.1898\n   12   StandardScalerWrapper ElasticNet               0:00:20             0.2017    0.1898\n   13   StandardScalerWrapper ElasticNet               0:00:22             0.2017    0.1898\n   14   StandardScalerWrapper ElasticNet               0:00:21             0.2024    0.1898\n   15   StandardScalerWrapper ElasticNet               0:00:21             0.2015    0.1898\n   16   StandardScalerWrapper LightGBM                 0:08:45             0.1886    0.1886\n   17   SparseNormalizer LightGBM                      0:04:57             0.2041    0.1886\n   18   StandardScalerWrapper RandomForest             0:01:04             0.2051    0.1886\n   19   StandardScalerWrapper ExtremeRandomTrees       0:29:59                nan    0.1886\n   20                                                  0:00:03                nan    0.1886\nERROR: {\n    \"additional_properties\": {},\n    \"error\": {\n        \"additional_properties\": {\n            \"debugInfo\": null\n        },\n        \"code\": \"UserError\",\n        \"severity\": null,\n        \"message\": \"Experiment timeout reached, please consider increasing your experiment timeout.\",\n        \"message_format\": \"Experiment timeout reached, please consider increasing your experiment timeout.\",\n        \"message_parameters\": {},\n        \"reference_code\": null,\n        \"details_uri\": null,\n        \"target\": null,\n        \"details\": [],\n        \"inner_error\": {\n            \"additional_properties\": {},\n            \"code\": \"ResourceExhausted\",\n            \"inner_error\": {\n                \"additional_properties\": {},\n                \"code\": \"Timeout\",\n                \"inner_error\": {\n                    \"additional_properties\": {},\n                    \"code\": \"ExperimentTimeoutForIterations\",\n                    \"inner_error\": null\n                }\n            }\n        },\n        \"additional_info\": null\n    },\n    \"correlation\": null,\n    \"environment\": null,\n    \"location\": null,\n    \"time\": {},\n    \"component_name\": null\n}\n   21                                                  0:00:03                nan    0.1886\nERROR: {\n    \"additional_properties\": {},\n    \"error\": {\n        \"additional_properties\": {\n            \"debugInfo\": null\n        },\n        \"code\": \"UserError\",\n        \"severity\": null,\n        \"message\": \"Experiment timeout reached, please consider increasing your experiment timeout.\",\n        \"message_format\": \"Experiment timeout reached, please consider increasing your experiment timeout.\",\n        \"message_parameters\": {},\n        \"reference_code\": null,\n        \"details_uri\": null,\n        \"target\": null,\n        \"details\": [],\n        \"inner_error\": {\n            \"additional_properties\": {},\n            \"code\": \"ResourceExhausted\",\n            \"inner_error\": {\n                \"additional_properties\": {},\n                \"code\": \"Timeout\",\n                \"inner_error\": {\n                    \"additional_properties\": {},\n                    \"code\": \"ExperimentTimeoutForIterations\",\n                    \"inner_error\": null\n                }\n            }\n        },\n        \"additional_info\": null\n    },\n    \"correlation\": null,\n    \"environment\": null,\n    \"location\": null,\n    \"time\": {},\n    \"component_name\": null\n}\n   22                                                  0:00:03                nan    0.1886\nERROR: {\n    \"additional_properties\": {},\n    \"error\": {\n        \"additional_properties\": {\n            \"debugInfo\": null\n        },\n        \"code\": \"UserError\",\n        \"severity\": null,\n        \"message\": \"Experiment timeout reached, please consider increasing your experiment timeout.\",\n        \"message_format\": \"Experiment timeout reached, please consider increasing your experiment timeout.\",\n        \"message_parameters\": {},\n        \"reference_code\": null,\n        \"details_uri\": null,\n        \"target\": null,\n        \"details\": [],\n        \"inner_error\": {\n            \"additional_properties\": {},\n            \"code\": \"ResourceExhausted\",\n            \"inner_error\": {\n                \"additional_properties\": {},\n                \"code\": \"Timeout\",\n                \"inner_error\": {\n                    \"additional_properties\": {},\n                    \"code\": \"ExperimentTimeoutForIterations\",\n                    \"inner_error\": null\n                }\n            }\n        },\n        \"additional_info\": null\n    },\n    \"correlation\": null,\n    \"environment\": null,\n    \"location\": null,\n    \"time\": {},\n    \"component_name\": null\n}\n   23    VotingEnsemble                                0:01:46             0.1883    0.1883\n   24    StackEnsemble                                 0:02:06             0.1881    0.1881\n"
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'tosequence' from 'sklearn.utils' (/anaconda/envs/azureml_py38/lib/python3.10/site-packages/sklearn/utils/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m compute_target \u001b[38;5;241m=\u001b[39m setup_compute_target(ws)\n\u001b[1;32m      4\u001b[0m automl_config \u001b[38;5;241m=\u001b[39m configure_automl(train_dataset, compute_target)\n\u001b[0;32m----> 5\u001b[0m automl_run, best_run, fitted_model \u001b[38;5;241m=\u001b[39m \u001b[43mrun_automl_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoml_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[9], line 11\u001b[0m, in \u001b[0;36mrun_automl_experiment\u001b[0;34m(ws, automl_config)\u001b[0m\n\u001b[1;32m      8\u001b[0m automl_run \u001b[38;5;241m=\u001b[39m experiment\u001b[38;5;241m.\u001b[39msubmit(automl_config, show_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, wait_post_processing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Get best model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m best_run, fitted_model \u001b[38;5;241m=\u001b[39m \u001b[43mautoml_run\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfitted_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest run metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/azureml/train/automl/run.py:718\u001b[0m, in \u001b[0;36mAutoMLRun.get_output\u001b[0;34m(self, iteration, metric, return_onnx_model, return_split_onnx_model, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m         fitted_model \u001b[38;5;241m=\u001b[39m _download_automl_onnx_model(curr_run, model_name)\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m         fitted_model \u001b[38;5;241m=\u001b[39m \u001b[43m_download_automl_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m curr_run, fitted_model\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/azureml/train/automl/_model_download_utilities.py:80\u001b[0m, in \u001b[0;36m_download_automl_model\u001b[0;34m(run, model_name)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ClientException(\u001b[38;5;28mstr\u001b[39m(e))\u001b[38;5;241m.\u001b[39mwith_generic_msg(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDownloading AutoML model failed.\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# Pass through any exceptions from loading the model.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Try is used here to ensure we can cleanup the side effect of model downlad.\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_automl_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;66;03m# If we can retrieve the automl runtime version, we do so we can inform the user what to install.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;66;03m# Otherwise just tell them to install latest runtime version (this is not an expected scenario).\u001b[39;00m\n\u001b[1;32m     84\u001b[0m         automl_runtime_ver \u001b[38;5;241m=\u001b[39m azureml_run_deps\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazureml-train-automl-runtime\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/azureml/train/automl/_model_download_utilities.py:172\u001b[0m, in \u001b[0;36m_load_automl_model\u001b[0;34m(model_path, suffix)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_model:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(model_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m model_file:\n\u001b[0;32m--> 172\u001b[0m         fitted_model \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: Optional[Any]\u001b[39;00m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fitted_model\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# Load the torch model with pytorch.\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/azureml/automl/runtime/featurization/__init__.py:8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"Init for featurization module.\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Data transformer\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_transformer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataTransformer, TransformerAndMapper\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeaturizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Featurizers\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric_transformer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GenericTransformer\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/azureml/automl/runtime/featurization/data_transformer.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sparse\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn_pandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataFrameMapper\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mazureml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_common\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_error_definition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureMLError\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mazureml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_common\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_error_definition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01muser_error\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArgumentBlankOrEmpty\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/sklearn_pandas/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1.7.0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe_mapper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataFrameMapper  \u001b[38;5;66;03m# NOQA\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cross_val_score, GridSearchCV, RandomizedSearchCV  \u001b[38;5;66;03m# NOQA\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcategorical_imputer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CategoricalImputer  \u001b[38;5;66;03m# NOQA\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/sklearn_pandas/dataframe_mapper.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseEstimator, TransformerMixin\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataWrapper\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_transformer_pipeline, _call_fit, TransformerPipeline\n\u001b[1;32m     12\u001b[0m PY3 \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mversion_info[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m PY3:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/sklearn_pandas/pipeline.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msix\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _name_estimators, Pipeline\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tosequence\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_call_fit\u001b[39m(fit_method, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    helper function, calls the fit or fit_transform method with the correct\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    number of parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    other TypeError\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'tosequence' from 'sklearn.utils' (/anaconda/envs/azureml_py38/lib/python3.10/site-packages/sklearn/utils/__init__.py)"
          ]
        }
      ],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1753895155608
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Step 6: Evaluate and deploy\n",
        "# print(\"\\n=== Step 6: Evaluating and Deploying Model ===\")\n",
        "# model, y_pred = evaluate_model(best_run, test_df)\n",
        "# registered_model, service = register_and_deploy_model(ws, best_run)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Libraries\n",
        "# !pip install --upgrade mlflow azureml-mlflow"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: mlflow in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (3.1.4)\nRequirement already satisfied: azureml-mlflow in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (1.60.0)\nCollecting azureml-mlflow\n  Using cached azureml_mlflow-1.60.0.post1-py3-none-any.whl (1.0 MB)\nCollecting azure.ai.ml\n  Downloading azure_ai_ml-1.28.1-py3-none-any.whl (13.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (2.0.42)\nRequirement already satisfied: scipy<2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (1.11.0)\nRequirement already satisfied: pyarrow<21,>=4.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (14.0.2)\nRequirement already satisfied: scikit-learn<2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (1.7.1)\nRequirement already satisfied: Flask<4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (3.0.3)\nRequirement already satisfied: pandas<3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (1.5.3)\nRequirement already satisfied: docker<8,>=4.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (7.1.0)\nRequirement already satisfied: matplotlib<4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (3.7.1)\nRequirement already satisfied: gunicorn<24 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (23.0.0)\nRequirement already satisfied: numpy<3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (1.23.5)\nRequirement already satisfied: alembic!=1.10.0,<2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (1.16.4)\nRequirement already satisfied: mlflow-skinny==3.1.4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (3.1.4)\nRequirement already satisfied: graphene<4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (3.4.3)\nRequirement already satisfied: packaging<26 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (25.0)\nRequirement already satisfied: protobuf<7,>=3.12.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (4.25.8)\nRequirement already satisfied: uvicorn<1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (0.34.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (1.35.0)\nRequirement already satisfied: click<9,>=7.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (8.1.8)\nRequirement already satisfied: databricks-sdk<1,>=0.20.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (0.49.0)\nRequirement already satisfied: cloudpickle<4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (2.2.1)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (3.1.44)\nRequirement already satisfied: cachetools<7,>=5.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (5.5.2)\nRequirement already satisfied: pyyaml<7,>=5.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (6.0.2)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (0.5.3)\nRequirement already satisfied: typing-extensions<5,>=4.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (4.14.1)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (1.35.0)\nRequirement already satisfied: fastapi<1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (0.115.12)\nRequirement already satisfied: pydantic<3,>=1.10.8 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (2.9.2)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (8.2.0)\nRequirement already satisfied: requests<3,>=2.17.3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (2.32.4)\nRequirement already satisfied: azure-common<2.0.0,>=1.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-mlflow) (1.1.28)\nRequirement already satisfied: azure-mgmt-core<2.0.0,>=1.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-mlflow) (1.5.0)\nRequirement already satisfied: pytz in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-mlflow) (2022.5)\nRequirement already satisfied: azure-storage-blob<=12.19.0,>=12.5.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-mlflow) (12.19.0)\nRequirement already satisfied: msrest>=0.6.18 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-mlflow) (0.7.1)\nRequirement already satisfied: azure-identity in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-mlflow) (1.21.0)\nRequirement already satisfied: azure-core!=1.22.0,<2.0.0,>=1.8.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-mlflow) (1.33.0)\nRequirement already satisfied: cryptography in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-mlflow) (38.0.4)\nRequirement already satisfied: jsonpickle in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-mlflow) (4.0.5)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.7.3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-mlflow) (2.9.0.post0)\nCollecting azure-storage-file-datalake>=12.2.0\n  Downloading azure_storage_file_datalake-12.21.0-py3-none-any.whl (264 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.1/264.1 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: six>=1.11.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure.ai.ml) (1.17.0)\nRequirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure.ai.ml) (4.24.0)\nCollecting pydash<9.0.0,>=6.0.0\n  Downloading pydash-8.0.5-py3-none-any.whl (102 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.1/102.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: colorama<1.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure.ai.ml) (0.4.6)\nRequirement already satisfied: tqdm<5.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure.ai.ml) (4.67.1)\nCollecting azure-monitor-opentelemetry\n  Downloading azure_monitor_opentelemetry-1.6.13-py3-none-any.whl (25 kB)\nCollecting azure-storage-file-share\n  Downloading azure_storage_file_share-12.22.0-py3-none-any.whl (291 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.3/291.3 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: isodate<1.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure.ai.ml) (0.7.2)\nCollecting marshmallow<4.0.0,>=3.5\n  Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyjwt<3.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure.ai.ml) (2.4.0)\nCollecting strictyaml<2.0.0\n  Downloading strictyaml-1.7.3-py3-none-any.whl (123 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.9/123.9 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: Mako in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\nRequirement already satisfied: tomli in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (2.2.1)\nCollecting azure-storage-file-datalake>=12.2.0\n  Downloading azure_storage_file_datalake-12.20.0-py3-none-any.whl (263 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.0/264.0 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading azure_storage_file_datalake-12.19.0-py3-none-any.whl (256 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.0/257.0 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading azure_storage_file_datalake-12.18.1-py3-none-any.whl (258 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.3/258.3 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading azure_storage_file_datalake-12.18.0-py3-none-any.whl (258 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.4/258.4 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading azure_storage_file_datalake-12.17.0-py3-none-any.whl (255 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.7/255.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading azure_storage_file_datalake-12.16.0-py3-none-any.whl (255 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.6/255.6 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading azure_storage_file_datalake-12.15.0-py3-none-any.whl (254 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.3/254.3 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading azure_storage_file_datalake-12.14.0-py3-none-any.whl (251 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.0/251.0 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: cffi>=1.12 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from cryptography->azureml-mlflow) (1.17.1)\nRequirement already satisfied: urllib3>=1.26.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\nRequirement already satisfied: Jinja2>=3.1.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from Flask<4->mlflow) (3.1.6)\nRequirement already satisfied: blinker>=1.6.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from Flask<4->mlflow) (1.9.0)\nRequirement already satisfied: itsdangerous>=2.1.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from Flask<4->mlflow) (2.1.2)\nRequirement already satisfied: Werkzeug>=3.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from Flask<4->mlflow) (3.1.3)\nRequirement already satisfied: graphql-relay<3.3,>=3.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from graphene<4->mlflow) (3.2.0)\nRequirement already satisfied: graphql-core<3.3,>=3.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from graphene<4->mlflow) (3.2.6)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->azure.ai.ml) (2025.4.1)\nRequirement already satisfied: attrs>=22.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->azure.ai.ml) (25.3.0)\nRequirement already satisfied: rpds-py>=0.7.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->azure.ai.ml) (0.24.0)\nRequirement already satisfied: referencing>=0.28.4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->azure.ai.ml) (0.36.2)\nRequirement already satisfied: pillow>=6.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from matplotlib<4->mlflow) (9.2.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from matplotlib<4->mlflow) (3.2.3)\nRequirement already satisfied: fonttools>=4.22.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from matplotlib<4->mlflow) (4.51.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.4.8)\nRequirement already satisfied: contourpy>=1.0.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from matplotlib<4->mlflow) (0.12.1)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from msrest>=0.6.18->azureml-mlflow) (2025.7.9)\nRequirement already satisfied: requests-oauthlib>=0.5.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from msrest>=0.6.18->azureml-mlflow) (2.0.0)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (3.6.0)\nRequirement already satisfied: joblib>=1.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (1.2.0)\nRequirement already satisfied: greenlet>=1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\nRequirement already satisfied: msal-extensions>=1.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-identity->azureml-mlflow) (1.2.0)\nRequirement already satisfied: msal>=1.30.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-identity->azureml-mlflow) (1.33.0b1)\nCollecting opentelemetry-sdk<3,>=1.9.0\n  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting opentelemetry-instrumentation-fastapi~=0.57b0\n  Downloading opentelemetry_instrumentation_fastapi-0.57b0-py3-none-any.whl (12 kB)\nCollecting azure-core-tracing-opentelemetry~=1.0.0b11\n  Downloading azure_core_tracing_opentelemetry-1.0.0b12-py3-none-any.whl (11 kB)\nCollecting opentelemetry-instrumentation-flask~=0.57b0\n  Downloading opentelemetry_instrumentation_flask-0.57b0-py3-none-any.whl (14 kB)\nCollecting opentelemetry-resource-detector-azure~=0.1.5\n  Downloading opentelemetry_resource_detector_azure-0.1.5-py3-none-any.whl (14 kB)\nCollecting opentelemetry-instrumentation-urllib3~=0.57b0\n  Downloading opentelemetry_instrumentation_urllib3-0.57b0-py3-none-any.whl (13 kB)\nCollecting azure-monitor-opentelemetry-exporter~=1.0.0b40\n  Downloading azure_monitor_opentelemetry_exporter-1.0.0b40-py2.py3-none-any.whl (159 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.0/160.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting opentelemetry-instrumentation-psycopg2~=0.57b0\n  Downloading opentelemetry_instrumentation_psycopg2-0.57b0-py3-none-any.whl (10 kB)\nCollecting opentelemetry-instrumentation-requests~=0.57b0\n  Downloading opentelemetry_instrumentation_requests-0.57b0-py3-none-any.whl (12 kB)\nCollecting opentelemetry-instrumentation-django~=0.57b0\n  Downloading opentelemetry_instrumentation_django-0.57b0-py3-none-any.whl (19 kB)\nCollecting opentelemetry-instrumentation-urllib~=0.57b0\n  Downloading opentelemetry_instrumentation_urllib-0.57b0-py3-none-any.whl (12 kB)\nCollecting psutil<8,>=5.9\n  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fixedint==0.1.6\n  Downloading fixedint-0.1.6-py3-none-any.whl (12 kB)\nRequirement already satisfied: pycparser in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from cffi>=1.12->cryptography->azureml-mlflow) (2.22)\nRequirement already satisfied: google-auth~=2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (2.38.0)\nRequirement already satisfied: starlette<0.47.0,>=0.40.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from fastapi<1->mlflow-skinny==3.1.4->mlflow) (0.46.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.1.4->mlflow) (4.0.12)\nRequirement already satisfied: zipp>=0.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.4->mlflow) (3.19.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from Jinja2>=3.1.2->Flask<4->mlflow) (3.0.2)\nRequirement already satisfied: portalocker<3,>=1.4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from msal-extensions>=1.2.0->azure-identity->azureml-mlflow) (2.10.1)\nCollecting opentelemetry-instrumentation==0.57b0\n  Downloading opentelemetry_instrumentation-0.57b0-py3-none-any.whl (32 kB)\nCollecting opentelemetry-semantic-conventions==0.57b0\n  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting opentelemetry-util-http==0.57b0\n  Downloading opentelemetry_util_http-0.57b0-py3-none-any.whl (7.6 kB)\nCollecting opentelemetry-instrumentation-wsgi==0.57b0\n  Downloading opentelemetry_instrumentation_wsgi-0.57b0-py3-none-any.whl (14 kB)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.57b0->opentelemetry-instrumentation-django~=0.57b0->azure-monitor-opentelemetry->azure.ai.ml) (1.14.1)\nCollecting opentelemetry-api<3,>=1.9.0\n  Downloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.57b0\n  Downloading opentelemetry_instrumentation_asgi-0.57b0-py3-none-any.whl (16 kB)\nCollecting asgiref~=3.0\n  Downloading asgiref-3.9.1-py3-none-any.whl (23 kB)\nCollecting opentelemetry-instrumentation-dbapi==0.57b0\n  Downloading opentelemetry_instrumentation_dbapi-0.57b0-py3-none-any.whl (12 kB)\nRequirement already satisfied: pydantic-core==2.23.4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.4->mlflow) (2.23.4)\nRequirement already satisfied: annotated-types>=0.6.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.4->mlflow) (0.7.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.4->mlflow) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.4->mlflow) (3.10)\nRequirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.18->azureml-mlflow) (3.2.2)\nRequirement already satisfied: h11>=0.8 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from uvicorn<1->mlflow-skinny==3.1.4->mlflow) (0.16.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.1.4->mlflow) (5.0.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (4.9)\nRequirement already satisfied: anyio<5,>=3.6.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.4->mlflow) (4.9.0)\nRequirement already satisfied: sniffio>=1.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.4->mlflow) (1.3.1)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.4->mlflow) (1.3.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (0.6.1)\nInstalling collected packages: fixedint, pydash, psutil, opentelemetry-util-http, marshmallow, asgiref, strictyaml, opentelemetry-api, opentelemetry-semantic-conventions, azure-storage-file-share, azure-core-tracing-opentelemetry, opentelemetry-sdk, opentelemetry-instrumentation, azure-storage-file-datalake, opentelemetry-resource-detector-azure, opentelemetry-instrumentation-wsgi, opentelemetry-instrumentation-urllib3, opentelemetry-instrumentation-urllib, opentelemetry-instrumentation-requests, opentelemetry-instrumentation-dbapi, opentelemetry-instrumentation-asgi, opentelemetry-instrumentation-psycopg2, opentelemetry-instrumentation-flask, opentelemetry-instrumentation-fastapi, opentelemetry-instrumentation-django, azure-monitor-opentelemetry-exporter, azure-monitor-opentelemetry, azure.ai.ml\n  Attempting uninstall: psutil\n    Found existing installation: psutil 5.2.2\n    Uninstalling psutil-5.2.2:\n      Successfully uninstalled psutil-5.2.2\n  Attempting uninstall: opentelemetry-api\n    Found existing installation: opentelemetry-api 1.35.0\n    Uninstalling opentelemetry-api-1.35.0:\n      Successfully uninstalled opentelemetry-api-1.35.0\n  Attempting uninstall: opentelemetry-semantic-conventions\n    Found existing installation: opentelemetry-semantic-conventions 0.56b0\n    Uninstalling opentelemetry-semantic-conventions-0.56b0:\n      Successfully uninstalled opentelemetry-semantic-conventions-0.56b0\n  Attempting uninstall: opentelemetry-sdk\n    Found existing installation: opentelemetry-sdk 1.35.0\n    Uninstalling opentelemetry-sdk-1.35.0:\n      Successfully uninstalled opentelemetry-sdk-1.35.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nopentelemetry-exporter-prometheus 0.56b0 requires opentelemetry-sdk~=1.35.0, but you have opentelemetry-sdk 1.36.0 which is incompatible.\njupyterlab-nvdashboard 0.13.0 requires jupyterlab>=4, but you have jupyterlab 3.6.8 which is incompatible.\njupyter-resource-usage 0.7.2 requires psutil~=5.6, but you have psutil 7.0.0 which is incompatible.\ndask-sql 2024.5.0 requires dask[dataframe]>=2024.4.1, but you have dask 2023.2.0 which is incompatible.\ndask-sql 2024.5.0 requires distributed>=2024.4.1, but you have distributed 2023.2.0 which is incompatible.\nazureml-training-tabular 1.60.0 requires psutil<5.9.4,>=5.2.2, but you have psutil 7.0.0 which is incompatible.\nazureml-training-tabular 1.60.0 requires scikit-learn<=1.6,>=1.0.0, but you have scikit-learn 1.7.1 which is incompatible.\nazureml-training-tabular 1.60.0 requires scipy<1.11.0,>=1.0.0, but you have scipy 1.11.0 which is incompatible.\nazureml-training-tabular 1.60.0 requires urllib3<2.0.0, but you have urllib3 2.5.0 which is incompatible.\nazureml-automl-runtime 1.60.0 requires psutil<5.9.4,>=5.2.2, but you have psutil 7.0.0 which is incompatible.\nazureml-automl-runtime 1.60.0 requires scikit-learn~=1.5.1, but you have scikit-learn 1.7.1 which is incompatible.\nazureml-automl-runtime 1.60.0 requires urllib3<2.0.0, but you have urllib3 2.5.0 which is incompatible.\nazureml-automl-dnn-nlp 1.60.0 requires torch==2.2.2, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed asgiref-3.9.1 azure-core-tracing-opentelemetry-1.0.0b12 azure-monitor-opentelemetry-1.6.13 azure-monitor-opentelemetry-exporter-1.0.0b40 azure-storage-file-datalake-12.14.0 azure-storage-file-share-12.22.0 azure.ai.ml-1.28.1 fixedint-0.1.6 marshmallow-3.26.1 opentelemetry-api-1.36.0 opentelemetry-instrumentation-0.57b0 opentelemetry-instrumentation-asgi-0.57b0 opentelemetry-instrumentation-dbapi-0.57b0 opentelemetry-instrumentation-django-0.57b0 opentelemetry-instrumentation-fastapi-0.57b0 opentelemetry-instrumentation-flask-0.57b0 opentelemetry-instrumentation-psycopg2-0.57b0 opentelemetry-instrumentation-requests-0.57b0 opentelemetry-instrumentation-urllib-0.57b0 opentelemetry-instrumentation-urllib3-0.57b0 opentelemetry-instrumentation-wsgi-0.57b0 opentelemetry-resource-detector-azure-0.1.5 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 opentelemetry-util-http-0.57b0 psutil-7.0.0 pydash-8.0.5 strictyaml-1.7.3\n"
        }
      ],
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1753899491343
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's verify the MLflow tracking URI and run\n",
        "print(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n",
        "    \n",
        "# Initialize the MLflow client\n",
        "client = MlflowClient()\n",
        "    \n",
        "# Get the current active run or specify a run_id\n",
        "run_id = \"AutoML_52c2a35d-2d37-434d-bdf3-dbe469298ed6_24\"  # You need to replace this\n",
        "print(f\"Using specified run ID: {run_id}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "MLflow Tracking URI: azureml://centralus.api.azureml.ms/mlflow/v2.0/subscriptions/c266be78-fe83-4597-afd1-5a4ea8eda621/resourceGroups/AzureML/providers/Microsoft.MachineLearningServices/workspaces/AzureML\nUsing specified run ID: AutoML_52c2a35d-2d37-434d-bdf3-dbe469298ed6_24\n"
        }
      ],
      "execution_count": 56,
      "metadata": {
        "gather": {
          "logged": 1753908308586
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_info = client.get_run(run_id)\n",
        "print(f\"Run status: {run_info.info.status}\")\n",
        "print(f\"Run lifecycle_stage: {run_info.info.lifecycle_stage}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "WARNING:urllib3.connectionpool:Retrying (Retry(total=6, connect=7, read=6, redirect=7, status=7)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='centralus.api.azureml.ms', port=443): Read timed out. (read timeout=120)\")': /mlflow/v2.0/subscriptions/c266be78-fe83-4597-afd1-5a4ea8eda621/resourceGroups/AzureML/providers/Microsoft.MachineLearningServices/workspaces/AzureML/api/2.0/mlflow/runs/get?run_uuid=AutoML_52c2a35d-2d37-434d-bdf3-dbe469298ed6_24&run_id=AutoML_52c2a35d-2d37-434d-bdf3-dbe469298ed6_24\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Run status: FINISHED\nRun lifecycle_stage: active\n"
        }
      ],
      "execution_count": 57,
      "metadata": {
        "gather": {
          "logged": 1753908434875
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "local_dir = \"./artifact_downloads\"\n",
        "local_path = client.download_artifacts(run_id, \"\", local_dir)\n",
        "print(f\"All artifacts downloaded to: {local_path}\")"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "azureml_artifacts_builder() takes from 0 to 1 positional arguments but 2 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[59], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m local_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./artifact_downloads\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m local_path \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll artifacts downloaded to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/mlflow/tracking/client.py:3378\u001b[0m, in \u001b[0;36mMlflowClient.download_artifacts\u001b[0;34m(self, run_id, path, dst_path)\u001b[0m\n\u001b[1;32m   3331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdownload_artifacts\u001b[39m(\u001b[38;5;28mself\u001b[39m, run_id: \u001b[38;5;28mstr\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m, dst_path: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m   3332\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3333\u001b[0m \u001b[38;5;124;03m    Download an artifact file or directory from a run to a local directory if applicable,\u001b[39;00m\n\u001b[1;32m   3334\u001b[0m \u001b[38;5;124;03m    and return a local path for it.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3376\u001b[0m \u001b[38;5;124;03m        Artifacts: ['features.txt']\u001b[39;00m\n\u001b[1;32m   3377\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tracking_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:715\u001b[0m, in \u001b[0;36mTrackingServiceClient.download_artifacts\u001b[0;34m(self, run_id, path, dst_path)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Download an artifact file or directory from a run to a local directory if applicable,\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;124;03mand return a local path for it.\u001b[39;00m\n\u001b[1;32m    699\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    711\u001b[0m \n\u001b[1;32m    712\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmlflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifacts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m download_artifacts\n\u001b[0;32m--> 715\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdownload_artifacts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdst_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracking_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtracking_uri\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/mlflow/artifacts/__init__.py:98\u001b[0m, in \u001b[0;36mdownload_artifacts\u001b[0;34m(artifact_uri, run_id, artifact_path, dst_path, tracking_uri)\u001b[0m\n\u001b[1;32m     96\u001b[0m store \u001b[38;5;241m=\u001b[39m _get_store(store_uri\u001b[38;5;241m=\u001b[39mtracking_uri)\n\u001b[1;32m     97\u001b[0m artifact_uri \u001b[38;5;241m=\u001b[39m store\u001b[38;5;241m.\u001b[39mget_run(run_id)\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39martifact_uri\n\u001b[0;32m---> 98\u001b[0m artifact_repo \u001b[38;5;241m=\u001b[39m \u001b[43mget_artifact_repository\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_databricks_profile_info_to_artifact_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracking_uri\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracking_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtracking_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m artifact_repo\u001b[38;5;241m.\u001b[39mdownload_artifacts(artifact_path, dst_path\u001b[38;5;241m=\u001b[39mdst_path)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/mlflow/store/artifact/artifact_repository_registry.py:144\u001b[0m, in \u001b[0;36mget_artifact_repository\u001b[0;34m(artifact_uri, tracking_uri)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_artifact_repository\u001b[39m(\n\u001b[1;32m    128\u001b[0m     artifact_uri: \u001b[38;5;28mstr\u001b[39m, tracking_uri: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    129\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArtifactRepository:\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Get an artifact repository from the registry based on the scheme of artifact_uri\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m        requirements.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_artifact_repository_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_artifact_repository\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracking_uri\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/mlflow/store/artifact/artifact_repository_registry.py:82\u001b[0m, in \u001b[0;36mArtifactRepositoryRegistry.get_artifact_repository\u001b[0;34m(self, artifact_uri, tracking_uri)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repository \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find a registered artifact repository for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00martifact_uri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently registered schemes are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m     )\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrepository\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracking_uri\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mTypeError\u001b[0m: azureml_artifacts_builder() takes from 0 to 1 positional arguments but 2 were given"
          ]
        }
      ],
      "execution_count": 59,
      "metadata": {
        "gather": {
          "logged": 1753908640642
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#find the artifacts\n",
        "artifacts = client.list_artifacts(run_id)\n",
        "print(\"Available artifacts:\")\n",
        "for artifact in artifacts:\n",
        "    print(f\"  - {artifact.path} (is_dir: {artifact.is_dir})\")"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "azureml_artifacts_builder() takes from 0 to 1 positional arguments but 2 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[58], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#find the artifacts\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m artifacts \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable artifacts:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m artifact \u001b[38;5;129;01min\u001b[39;00m artifacts:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/mlflow/tracking/client.py:3324\u001b[0m, in \u001b[0;36mMlflowClient.list_artifacts\u001b[0;34m(self, run_id, path)\u001b[0m\n\u001b[1;32m   3270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlist_artifacts\u001b[39m(\u001b[38;5;28mself\u001b[39m, run_id: \u001b[38;5;28mstr\u001b[39m, path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[FileInfo]:\n\u001b[1;32m   3271\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"List the artifacts for a run.\u001b[39;00m\n\u001b[1;32m   3272\u001b[0m \n\u001b[1;32m   3273\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3322\u001b[0m \n\u001b[1;32m   3323\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tracking_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:679\u001b[0m, in \u001b[0;36mTrackingServiceClient.list_artifacts\u001b[0;34m(self, run_id, path)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"List the artifacts for a run.\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \n\u001b[1;32m    668\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    675\u001b[0m \n\u001b[1;32m    676\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmlflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifacts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_artifacts\n\u001b[0;32m--> 679\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlist_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracking_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtracking_uri\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/mlflow/artifacts/__init__.py:150\u001b[0m, in \u001b[0;36mlist_artifacts\u001b[0;34m(artifact_uri, run_id, artifact_path, tracking_uri)\u001b[0m\n\u001b[1;32m    148\u001b[0m store \u001b[38;5;241m=\u001b[39m _get_store(store_uri\u001b[38;5;241m=\u001b[39mtracking_uri)\n\u001b[1;32m    149\u001b[0m artifact_uri \u001b[38;5;241m=\u001b[39m store\u001b[38;5;241m.\u001b[39mget_run(run_id)\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39martifact_uri\n\u001b[0;32m--> 150\u001b[0m artifact_repo \u001b[38;5;241m=\u001b[39m \u001b[43mget_artifact_repository\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_databricks_profile_info_to_artifact_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracking_uri\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracking_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtracking_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m artifact_repo\u001b[38;5;241m.\u001b[39mlist_artifacts(artifact_path)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/mlflow/store/artifact/artifact_repository_registry.py:144\u001b[0m, in \u001b[0;36mget_artifact_repository\u001b[0;34m(artifact_uri, tracking_uri)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_artifact_repository\u001b[39m(\n\u001b[1;32m    128\u001b[0m     artifact_uri: \u001b[38;5;28mstr\u001b[39m, tracking_uri: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    129\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArtifactRepository:\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Get an artifact repository from the registry based on the scheme of artifact_uri\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m        requirements.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_artifact_repository_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_artifact_repository\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracking_uri\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/mlflow/store/artifact/artifact_repository_registry.py:82\u001b[0m, in \u001b[0;36mArtifactRepositoryRegistry.get_artifact_repository\u001b[0;34m(self, artifact_uri, tracking_uri)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repository \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find a registered artifact repository for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00martifact_uri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently registered schemes are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m     )\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrepository\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracking_uri\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mTypeError\u001b[0m: azureml_artifacts_builder() takes from 0 to 1 positional arguments but 2 were given"
          ]
        }
      ],
      "execution_count": 58,
      "metadata": {
        "gather": {
          "logged": 1753908459049
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_id = \"YOUR_RUN_ID\"  # You need to replace this\n",
        "print(f\"Using specified run ID: {run_id}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required libraries\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "# The workspace information from the previous experiment has been pre-filled for you.\n",
        "subscription_id = \"c266be78-fe83-4597-afd1-5a4ea8eda621\"\n",
        "resource_group = \"AzureML\"\n",
        "workspace_name = \"AzureML\"\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "ml_client = MLClient(credential, subscription_id, resource_group, workspace_name)\n",
        "workspace = ml_client.workspaces.get(name=ml_client.workspace_name)\n",
        "print(ml_client.workspace_name, workspace.resource_group, workspace.location, ml_client.connections._subscription_id, sep = '\\n')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "AzureML\nAzureML\ncentralus\nc266be78-fe83-4597-afd1-5a4ea8eda621\n"
        }
      ],
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1753899542954
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction - using ML Flow - https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-mlflow-models-online-endpoints?view=azureml-api-2&tabs=mlflow\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from mlflow.deployments import get_deploy_client\n",
        "from mlflow.tracking import MlflowClient\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from azureml.core import Workspace, Model\n",
        "import os\n",
        "import logging"
      ],
      "outputs": [],
      "execution_count": 36,
      "metadata": {
        "gather": {
          "logged": 1753901281882
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import MLClient, Input\n",
        "from azure.ai.ml.entities import Model\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.identity import DefaultAzureCredential"
      ],
      "outputs": [],
      "execution_count": 43,
      "metadata": {
        "gather": {
          "logged": 1753903908630
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The workspace information from the previous experiment has been pre-filled for you.\n",
        "subscription_id = \"c266be78-fe83-4597-afd1-5a4ea8eda621\"\n",
        "resource_group = \"AzureML\"\n",
        "workspace_name = \"AzureML\"\n",
        "\n",
        "ml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\nWARNING:opentelemetry._logs._internal:Overriding of current LoggerProvider is not allowed\nWARNING:opentelemetry.metrics._internal:Overriding of current MeterProvider is not allowed\nWARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\nWARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\nWARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\nWARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\nWARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\nWARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n"
        }
      ],
      "execution_count": 44,
      "metadata": {
        "gather": {
          "logged": 1753903942123
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow_client = MlflowClient()"
      ],
      "outputs": [],
      "execution_count": 37,
      "metadata": {
        "gather": {
          "logged": 1753901304135
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n",
        "\n",
        "deployment_client = get_deploy_client(mlflow.get_tracking_uri())    \n",
        "print(deployment_client)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "MLflow Tracking URI: azureml://centralus.api.azureml.ms/mlflow/v2.0/subscriptions/c266be78-fe83-4597-afd1-5a4ea8eda621/resourceGroups/AzureML/providers/Microsoft.MachineLearningServices/workspaces/AzureML\n<azureml.mlflow.deploy.deployment_client.AzureMLDeploymentClient object at 0x7262e94b5ba0>\n"
        }
      ],
      "execution_count": 42,
      "metadata": {
        "gather": {
          "logged": 1753902342384
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Library and Create a local folder\n",
        "import os\n",
        "\n",
        "# Create local folder\n",
        "local_dir = \"./artifact_downloads\"\n",
        "\n",
        "if not os.path.exists(local_dir):\n",
        "    os.makedirs(local_dir)"
      ],
      "outputs": [],
      "execution_count": 48,
      "metadata": {
        "gather": {
          "logged": 1753905589674
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List all artifacts at the top level of the run\n",
        "artifacts = mlflow.artifacts.list_artifacts(run_id=run_id)\n",
        "\n",
        "print(f\"Artifacts in run {run_id}:\")\n",
        "for artifact in artifacts:\n",
        "    print(f\"- {artifact.path}\")\n",
        "\n",
        "\n",
        "    azureml.datastoreId : /subscriptions/c266be78-fe83-4597-afd1-5a4ea8eda621/resourceGroups/AzureML/providers/Microsoft.MachineLearningServices/workspaces/AzureML/datastores/workspaceartifactstore\n"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "azureml_artifacts_builder() takes from 0 to 1 positional arguments but 2 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# List all artifacts at the top level of the run\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m artifacts \u001b[38;5;241m=\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43martifacts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArtifacts in run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m artifact \u001b[38;5;129;01min\u001b[39;00m artifacts:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/mlflow/artifacts/__init__.py:150\u001b[0m, in \u001b[0;36mlist_artifacts\u001b[0;34m(artifact_uri, run_id, artifact_path, tracking_uri)\u001b[0m\n\u001b[1;32m    148\u001b[0m store \u001b[38;5;241m=\u001b[39m _get_store(store_uri\u001b[38;5;241m=\u001b[39mtracking_uri)\n\u001b[1;32m    149\u001b[0m artifact_uri \u001b[38;5;241m=\u001b[39m store\u001b[38;5;241m.\u001b[39mget_run(run_id)\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39martifact_uri\n\u001b[0;32m--> 150\u001b[0m artifact_repo \u001b[38;5;241m=\u001b[39m \u001b[43mget_artifact_repository\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_databricks_profile_info_to_artifact_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracking_uri\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracking_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtracking_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m artifact_repo\u001b[38;5;241m.\u001b[39mlist_artifacts(artifact_path)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/mlflow/store/artifact/artifact_repository_registry.py:144\u001b[0m, in \u001b[0;36mget_artifact_repository\u001b[0;34m(artifact_uri, tracking_uri)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_artifact_repository\u001b[39m(\n\u001b[1;32m    128\u001b[0m     artifact_uri: \u001b[38;5;28mstr\u001b[39m, tracking_uri: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    129\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArtifactRepository:\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Get an artifact repository from the registry based on the scheme of artifact_uri\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m        requirements.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_artifact_repository_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_artifact_repository\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracking_uri\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/mlflow/store/artifact/artifact_repository_registry.py:82\u001b[0m, in \u001b[0;36mArtifactRepositoryRegistry.get_artifact_repository\u001b[0;34m(self, artifact_uri, tracking_uri)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repository \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find a registered artifact repository for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00martifact_uri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently registered schemes are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m     )\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrepository\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracking_uri\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mTypeError\u001b[0m: azureml_artifacts_builder() takes from 0 to 1 positional arguments but 2 were given"
          ]
        }
      ],
      "execution_count": 47,
      "metadata": {
        "gather": {
          "logged": 1753904777580
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'hybrid-recommendation-automl'\n",
        "RUN_ID = 'AutoML_52c2a35d-2d37-434d-bdf3-dbe469298ed6_24'\n",
        "\n",
        "registered_model = mlflow_client.create_model_version(\n",
        "    name=model_name, source=f\"runs://{RUN_ID}/{MODEL_PATH}\"\n",
        ")\n",
        "version = registered_model.version"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'MODEL_PATH' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[41], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhybrid-recommendation-automl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m RUN_ID \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoML_52c2a35d-2d37-434d-bdf3-dbe469298ed6_24\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m registered_model \u001b[38;5;241m=\u001b[39m mlflow_client\u001b[38;5;241m.\u001b[39mcreate_model_version(\n\u001b[0;32m----> 5\u001b[0m     name\u001b[38;5;241m=\u001b[39mmodel_name, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRUN_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mMODEL_PATH\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m version \u001b[38;5;241m=\u001b[39m registered_model\u001b[38;5;241m.\u001b[39mversion\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MODEL_PATH' is not defined"
          ]
        }
      ],
      "execution_count": 41,
      "metadata": {
        "gather": {
          "logged": 1753901889873
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.10 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}